---
layout: post
author: Équipe A07
title: Modèle avancées - XGBoost
---

<style>
  #plot-container {
    justify-content: center;
    align-items: center;
    width: 60vw; 
    height: 60vh;
    margin-bottom: 0px;
  }

  table {
    width: 100%;
    border-collapse: collapse;
  }


  td, th {
    border: 1px solid #dddddd;
    text-align: left;
    padding: 8px;
  }

  tr:nth-child(even) {
    background-color: #f2f2f2;
  }

</style>

# Introduction 

Dans ce rapport, nous nous concentrons sur l'élaboration et la comparaison de deux modèles avancés XGBoost, une approche de pointe en matière de machine learning. Le premier modèle englobe l'ensemble des caractéristiques développées, y compris celles issues de la section sur l'ingénierie des caractéristiques II. Le second modèle se base exclusivement sur les caractéristiques sélectionnées dans l'article sur la sélection des caractéristiques. Ces deux modèles sont conçus pour surpasser les performances des modèles de base XGBoost établis précédemment. 

# XGBoost sur l'ensemble des caractéristiques

Dans notre approche, nous avons combiné deux stratégies principales : l'optimisation bayésienne des hyperparamètres et l'ajustement du poids des classes. Ces méthodes ont été appliquées aux deux modèles XGBoost développés pour améliorer leurs performances.

L'optimisation bayésienne des hyperparamètres est une méthode statistique qui vise à trouver la combinaison optimale d'hyperparamètres pour un modèle. Elle utilise un modèle probabiliste pour prédire la performance d'un modèle avec différents hyperparamètres et sélectionne ceux qui maximisent cette performance. Cette approche est particulièrement efficace pour gérer les grands espaces de paramètres et trouver le meilleur équilibre entre exploration et exploitation.

Dans le cadre de notre projet, nous avons ciblé spécifiquement les hyperparamètres suivants pour l'optimisation :

- **`min_child_weight`** : Ce paramètre, de type entier et variant entre 0 et 10, joue un rôle crucial dans la prévention du surapprentissage. Il détermine le nombre minimum de poids requis dans un enfant, c'est-à-dire dans un nœud résultant d'une division dans l'arbre de décision. Des valeurs plus élevées rendent le modèle plus conservateur en évitant des divisions trop spécifiques qui pourraient mener à un modèle trop complexe pour les données.

- **`colsample_bytree`** : De type flottant et commençant à 0.5, ce paramètre spécifie la proportion de caractéristiques à utiliser pour construire chaque arbre. Une valeur inférieure signifie qu'un plus petit nombre de caractéristiques est sélectionné, ce qui peut aider à réduire le surapprentissage en limitant la complexité de chaque arbre individuel.

- **`max_depth`** : Il s'agit d'un paramètre entier dont les valeurs varient de 3 à 18. Ce paramètre détermine la profondeur maximale de chaque arbre. Une valeur plus élevée permet au modèle de mieux saisir les relations complexes dans les données, mais augmente également le risque de surapprentissage. Trouver le bon équilibre avec ce paramètre est essentiel pour obtenir un modèle précis et généralisable.

- **`max_leaves`** : Avec une gamme allant de 0 à 10, ce paramètre entier détermine le nombre maximal de feuilles, ou nœuds terminaux, dans chaque arbre. Contrôler ce nombre aide à réguler la complexité de chaque arbre, influençant directement la complexité globale du modèle.

Voici un graphique présentant les hyperparamètres et leurs performances associées:
![SCREENSHOT OF Comparaison des types de tirs](xgboost_advanced/2023-10-08-val_roc_auc_score.jpeg)

Les meilleures hyperparamètres que nous avons obtenus sont les suivants :
- colsample_bytree : 0.728.
- learning_rate : 0.01. 
- max_depth : 11. 
- max_leaves : 0 (ici cela veut dire un nombre iliimité de feuilles).
- min_child_weight : 10. 
- reg_lambda : 1. 
- subsample : 0.5.

# XGBoost sur les caractéristiques sélectionnées

Nous avons aussi élaboré un modèle XGBoost sur les caractéristiques sélectionnées dans l'article à ce sujet. 